{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell_forward_self(xt, a_prev, parameters):\n",
    "    Wax = parameters['Wax']\n",
    "    Waa = parameters['Waa']\n",
    "    Wya = parameters['Wya']\n",
    "    ba = parameters['ba']\n",
    "    by = parameters['by']\n",
    "\n",
    "    a_next = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, xt) + ba)\n",
    "    yt_pred = softmax(np.dot(Wya, a_next) + by)\n",
    "\n",
    "    cache = (a_next, a_prev, xt, parameters)\n",
    "\n",
    "    return a_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward_self(x, a0, parameters):\n",
    "    caches = []\n",
    "\n",
    "    n_x, m, T_x  = x.shape\n",
    "    n_y, n_a = parameters['Wya'].shape\n",
    "\n",
    "    a = np.zeros((n_a, m, T_x))\n",
    "    y_pred = np.zeros((n_y, m, T_x))\n",
    "\n",
    "    a_next = a0\n",
    "\n",
    "    for t in range(T_x):\n",
    "        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t], a_next, parameters)\n",
    "        a[:,:,t] = a_next\n",
    "        y_pred[:,:,t] = yt_pred\n",
    "        caches.append(cache)\n",
    "    \n",
    "    caches = (caches, x)# for back-propagation\n",
    "    return a, y_pred, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM\n",
    "def lstm_cell_forward_self(xt, a_prev, c_prev, parameters):\n",
    "    Wf = parameters['Wf'] #forget gate weight\n",
    "    bf = parameters['bf']\n",
    "\n",
    "    Wi = parameters['Wi'] #update gate weight\n",
    "    bi = parameters['bi']\n",
    "\n",
    "    Wc = parameters['Wc'] #candidate weight\n",
    "    bc = parameters['bc']\n",
    "\n",
    "    Wo = parameters['Wo'] #output gate weight\n",
    "    bo = parameters['bo']\n",
    "\n",
    "    Wy = parameters['Wy'] #prediction weight\n",
    "    by = parameters['by']\n",
    "\n",
    "    n_x, m = xt.shape\n",
    "    n_y, n_a = Wy.shape\n",
    "\n",
    "    concat = np.concatenate([a_prev, xt])\n",
    "\n",
    "    ft = sigmoid(np.dot(Wf, concat) + bf) # Forget gate\n",
    "    it = sigmoid(np.dot(Wi, concat) + bi) # Update gate\n",
    "    cct = np.tanh(np.dot(Wc, concat) + bc) # Candidate value\n",
    "    c_next = c_prev * ft + cct * it\n",
    "    ot = sigmoid(np.dot(Wo, concat) + bo) # Output gate\n",
    "    a_next = ot * (np.tanh(c_next))\n",
    "\n",
    "    yt_pred = softmax(np.dot(Wy, a_next) + by)\n",
    "\n",
    "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)\n",
    "\n",
    "    return a_next, c_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_forward_self(x, a0, parameters):\n",
    "    caches = []\n",
    "\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters['Wy'].shape\n",
    "\n",
    "    a = np.zeros((n_a, m, T_x))\n",
    "    c = np.zeros((n_a, m, T_x))\n",
    "    y = np.zeors((n_y, m , T_x))\n",
    "\n",
    "    a_next = a0\n",
    "    c_next = np.zeros((n_a, m))\n",
    "\n",
    "    for t in range(T_x):\n",
    "        xt = x[:,:,t]\n",
    "\n",
    "        a_next, c_next, yt, cache = lstm_cell_forward(xt, a_next, c_next, parameters)\n",
    "\n",
    "        a[:,:,t] = a_next\n",
    "        c[:,:,t] = c_next\n",
    "        y[:,:,t] = yt\n",
    "\n",
    "        caches.append(cache)\n",
    "\n",
    "    caches = (caches, x) # for back-propagation\n",
    "    return a, y, c, caches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
